{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519c87ee",
   "metadata": {},
   "source": [
    "# DL Lab Practical 4\n",
    "# Autoencoders\n",
    "\n",
    "### Name: Aashi Khanna\n",
    "### Roll No.: 02\n",
    "### Batch: E1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c514b2af",
   "metadata": {},
   "source": [
    "Write a program to implement auto encoders for both binary and real valued inputs. Clearly show the loss after each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e399b74",
   "metadata": {},
   "source": [
    "#### Method\n",
    "- feedforward -> back propagation\n",
    "- loss function at o/p for (binary = cross entropy) and (real = MSE)\n",
    "- activation = sigmoid\n",
    "- randomly initialize weights using array\n",
    "- o/p should be same as i/p (target)\n",
    "- part 1: find loss for 1 iteration\n",
    "- part 2: backpropagate to minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87085e0",
   "metadata": {},
   "source": [
    "# 1. Binary Inputs - 1,0,1,1,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "44b9e2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from random import seed\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46121214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631]}, {'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536]}]\n",
      "[{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243]}, {'weights': [0.21659939713061338, 0.4221165755827173, 0.029040787574867943, 0.22169166627303505]}, {'weights': [0.43788759365057206, 0.49581224138185065, 0.23308445025757263, 0.2308665415409843]}, {'weights': [0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876]}, {'weights': [0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177]}]\n"
     ]
    }
   ],
   "source": [
    "# Initialize a network\n",
    "#organize layers as arrays of dictionaries and treat the whole network as an array of layers.\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    output_layer = [{'weights':[random() for i in range(n_hidden)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    return network\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(5, 4, 5)\n",
    "for layer in network:\n",
    " print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1169ead5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output after 1 iteration: [0.7736622642886162, 0.627795528348159, 0.7148486403216591, 0.6837447645183264, 0.8335934249081884]\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation\n",
    "from math import exp\n",
    " \n",
    "# Calculate neuron activation for an input. This is basically W(inputs) + b, no bias yet.\n",
    "def activate(weights, inputs):\n",
    "    activation = 0 #since no bias is given\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation or h = sigmoid(W(inputs)) \n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "\tinputs = row\n",
    "\tfor layer in network:\n",
    "\t\tnew_inputs = [] #these are the inputs for our output layer\n",
    "\t\tfor neuron in layer: # we calculate output for each neuron in the layer\n",
    "\t\t\tactivation = activate(neuron['weights'], inputs)\n",
    "\t\t\tneuron['output'] = transfer(activation)\n",
    "\t\t\tnew_inputs.append(neuron['output'])\n",
    "\t\tinputs = new_inputs\n",
    "\treturn inputs\n",
    "\n",
    "# test forward propagation\n",
    "network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095]}, \n",
    "            {'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631]}, \n",
    "            {'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, \n",
    "            {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536]}], \n",
    "           [{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243]}, \n",
    "            {'weights': [0.21659939713061338, 0.4221165755827173, 0.029040787574867943, 0.22169166627303505]}, \n",
    "            {'weights': [0.43788759365057206, 0.49581224138185065, 0.23308445025757263, 0.2308665415409843]}, \n",
    "            {'weights': [0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876]}, \n",
    "            {'weights': [0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177]}]]\n",
    "\n",
    "row = [1, 0, 1, 1, 1] #our input\n",
    "output = forward_propagate(network, row)\n",
    "print(f\"Final output after 1 iteration: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d2a70e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs in hidden layer after sigmoid activation : [0.7600963630690886, 0.7911834922153114, 0.8320395219105193, 0.9287892578991231]\n",
      "Final Outputs after softmax activation : [0.23122091478031795, 0.11409570573831791, 0.16957870568155464, 0.14624767063693933, 0.33885700316287015]\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation using softmax as activation in o/p layer\n",
    "from math import exp\n",
    "from numpy import exp\n",
    " \n",
    "# Calculate neuron activation for an input. This is basically W(inputs) + b, no bias yet.\n",
    "def activate(weights, inputs):\n",
    "    activation = 0 #since no bias is given\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation or h = sigmoid(W(inputs)) for hidden layer\n",
    "def transfer_sigmoid(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "    \n",
    "# Forward propagate input to a hidden\n",
    "\n",
    "def forward_propagate_input(input_network, input_row):\n",
    "    inputs = input_row\n",
    "    for layer in input_network:\n",
    "        new_inputs = [] #these are the inputs for our output layer\n",
    "        for neuron in layer: # we calculate output for each neuron in the layer\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer_sigmoid(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# test forward propagation till hidden layer\n",
    "input_network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095]},\n",
    "            {'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631]}, \n",
    "            {'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, \n",
    "            {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536]}]]\n",
    "\n",
    "input_row = [1, 0, 1, 1, 1] #our input\n",
    "\n",
    "hidden_row = forward_propagate_input(input_network, input_row)\n",
    "print(f\"Outputs in hidden layer after sigmoid activation : {hidden_row}\")\n",
    "\n",
    "#Forward propagate till output\n",
    "def forward_propagate_hidden(hidden_network, hidden):\n",
    "    inputs = hidden #for o/p layer, o/p in hidden layer are inputs\n",
    "    for layer in hidden_network:\n",
    "        pre_outputs = [] \n",
    "        for neuron in layer: # we calculate output for each neuron in the layer\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['pre_activ'] = exp(activation)\n",
    "            pre_outputs.append(neuron['pre_activ'])\n",
    "        #print(pre_outputs)\n",
    "        Sum = sum(pre_outputs)\n",
    "        #print(Sum)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in pre_outputs:\n",
    "            output = i/Sum\n",
    "            outputs.append(output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "hidden_network = [[{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243]}, \n",
    "            {'weights': [0.21659939713061338, 0.4221165755827173, 0.029040787574867943, 0.22169166627303505]}, \n",
    "            {'weights': [0.43788759365057206, 0.49581224138185065, 0.23308445025757263, 0.2308665415409843]}, \n",
    "            {'weights': [0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876]}, \n",
    "            {'weights': [0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177]}]]\n",
    "\n",
    "output_row = forward_propagate_hidden(hidden_network, hidden_row)\n",
    "print(f\"Final Outputs after softmax activation : {output_row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a79fcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 1 iteration: 1.248690120897951\n"
     ]
    }
   ],
   "source": [
    "#Calculating loss (cross entropy)\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "# calculate binary cross entropy\n",
    "def binary_cross_entropy(actual, predicted):\n",
    "    sum_score = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_score += actual[i] * log(1e-15 + predicted[i])\n",
    "    mean_sum_score = 1.0 / len(actual) * sum_score\n",
    "    loss = -mean_sum_score\n",
    "    return loss\n",
    "\n",
    "error = binary_cross_entropy(input_row, output_row)\n",
    "print(f\"Loss after 1 iteration: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77040f2c",
   "metadata": {},
   "source": [
    "# 2. Real valued inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5a15861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.7312715117751976, 0.6948674738744653, 0.5275492379532281, -0.4898619485211566, -0.009129825816118098]\n"
     ]
    }
   ],
   "source": [
    "#generating random real valued inputs\n",
    "import random\n",
    "seed(1)\n",
    "\n",
    "real_inputs = []\n",
    "\n",
    "for i in range(0,5):\n",
    "    n = random.uniform(-1,1)\n",
    "    real_inputs.append(n)\n",
    "    \n",
    "print(real_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d22f4782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output after 1 iteration: [0.6994112292840425, 0.6047590633921736, 0.6745718057396166, 0.6425349261045457, 0.7779132459039804]\n"
     ]
    }
   ],
   "source": [
    "#using same weights as earlier and Forward propagating\n",
    "\n",
    "# Forward propagation\n",
    "from math import exp\n",
    " \n",
    "# Calculate neuron activation for an input. This is basically W(inputs) + b, no bias yet.\n",
    "def activate(weights, inputs):\n",
    "    activation = 0 #since no bias is given\n",
    "    for i in range(len(weights)-1):\n",
    "        activation += weights[i] * inputs[i]\n",
    "    return activation\n",
    "\n",
    "# Transfer neuron activation or h = sigmoid(W(inputs)) \n",
    "def transfer(activation):\n",
    "    return 1.0 / (1.0 + exp(-activation))\n",
    "\n",
    "\n",
    "# Forward propagate input to a network output\n",
    "def forward_propagate(network, row):\n",
    "    inputs = real_inputs\n",
    "    for layer in network:\n",
    "        new_inputs = [] #these are the inputs for our output layer\n",
    "        for neuron in layer: # we calculate output for each neuron in the layer\n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "    return inputs\n",
    "\n",
    "# test forward propagation\n",
    "real_network = [[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614, 0.2550690257394217, 0.49543508709194095]}, \n",
    "            {'weights': [0.4494910647887381, 0.651592972722763, 0.7887233511355132, 0.0938595867742349, 0.02834747652200631]}, \n",
    "            {'weights': [0.8357651039198697, 0.43276706790505337, 0.762280082457942, 0.0021060533511106927, 0.4453871940548014]}, \n",
    "            {'weights': [0.7215400323407826, 0.22876222127045265, 0.9452706955539223, 0.9014274576114836, 0.030589983033553536]}], \n",
    "           [{'weights': [0.0254458609934608, 0.5414124727934966, 0.9391491627785106, 0.38120423768821243]}, \n",
    "            {'weights': [0.21659939713061338, 0.4221165755827173, 0.029040787574867943, 0.22169166627303505]}, \n",
    "            {'weights': [0.43788759365057206, 0.49581224138185065, 0.23308445025757263, 0.2308665415409843]}, \n",
    "            {'weights': [0.2187810373376886, 0.4596034657377336, 0.28978161459048557, 0.021489705265908876]}, \n",
    "            {'weights': [0.8375779756625729, 0.5564543226524334, 0.6422943629324456, 0.1859062658947177]}]]\n",
    "\n",
    "real_outputs = forward_propagate(real_network, real_inputs)\n",
    "print(f\"Final output after 1 iteration: {real_outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ec31b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after 1st iteration: 0.7956695490095194\n"
     ]
    }
   ],
   "source": [
    "#Loss - Mean Squared Error\n",
    "# calculate mean squared error\n",
    "\n",
    "def mean_squared_error(actual, predicted):\n",
    "    sum_square_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        sum_square_error += (actual[i] - predicted[i])**2.0\n",
    "    mean_square_error = 1.0 / len(actual) * sum_square_error\n",
    "    return mean_square_error\n",
    "\n",
    "real_error = mean_squared_error(real_inputs, real_outputs)\n",
    "print(f\"Loss after 1st iteration: {real_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833f420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
